{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e090df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sandralm\\AppData\\Local\\miniconda3\\envs\\IDS\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import kaggle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle # to save training history\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b59ab23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './human-face-emotions/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9dc8df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 57756 files belonging to 5 classes.\n",
      "Classes: ['Angry', 'Fear', 'Happy', 'Sad', 'Suprise']\n"
     ]
    }
   ],
   "source": [
    "full_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    label_mode=\"int\",\n",
    "    image_size=(48, 48),\n",
    "    color_mode=\"grayscale\",\n",
    "    batch_size=None,      # return one (img, label) at a time\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "class_names = full_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(\"Classes:\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a06b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = list(full_ds.as_numpy_iterator())\n",
    "\n",
    "images = [x[0] for x in full_data]  # list of arrays\n",
    "labels = [x[1] for x in full_data]  # list of ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "943c4d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First split: train vs temp (val+test)\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    images, labels,\n",
    "    test_size=0.30,         # 30% → val+test\n",
    "    random_state=42,\n",
    "    stratify=labels         # keeps class proportions\n",
    ")\n",
    "\n",
    "# Second split: val vs test (each = 15%)\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp,\n",
    "    test_size=0.50,         # half of 30% = 15%\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ba4b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_ds   = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_ds  = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d455715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no doing this for transformer\n",
    "#batch_size = 128\n",
    "\n",
    "#train_ds = train_ds.shuffle(10000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "#val_ds   = val_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "#test_ds  = test_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT required size is 224x224\n",
    "\n",
    "def preprocess_vit(image, label):\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.resize(image, (224, 224))\n",
    "\n",
    "    # Normalize pixel values\n",
    "    image = image / 255.0\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## originally created for vit but same works for ef\n",
    "\n",
    "vit_train_ds = train_ds.map(preprocess_vit).shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "vit_val_ds   = val_ds.map(preprocess_vit).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "vit_test_ds  = test_ds.map(preprocess_vit).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "## prefetch(tf.data.AUTOTUNE) - makes preloading dynamic --> faster training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3506257",
   "metadata": {},
   "source": [
    "### self trained VIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f77bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, patch_size=4, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.projection = tf.keras.layers.Conv2D(\n",
    "            filters=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            strides=patch_size,\n",
    "            padding=\"valid\"\n",
    "        )\n",
    "        self.flatten = tf.keras.layers.Reshape((-1, embed_dim))\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = self.flatten(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e0e7325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len=1024, embed_dim=64):\n",
    "        super().__init__()\n",
    "        self.pos_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=max_len,\n",
    "            output_dim=embed_dim\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]              # symbolic-safe\n",
    "        positions = tf.range(seq_len)         # symbolic-safe\n",
    "        pos_embed = self.pos_embedding(positions)\n",
    "        return x + pos_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1af22c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization()\n",
    "        self.attn = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embed_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization()\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "            tf.keras.layers.Dropout(dropout),\n",
    "            tf.keras.layers.Dense(embed_dim),\n",
    "            tf.keras.layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x + self.attn(x, x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.mlp(x)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d71a5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_small_vit(input_shape=(48, 48, 1), num_classes=5):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # grayscale → RGB\n",
    "    x = tf.keras.layers.Lambda(lambda t: tf.concat([t, t, t], axis=-1))(inputs)\n",
    "\n",
    "    # resize to something divisible by 4\n",
    "    x = tf.keras.layers.Resizing(64, 64)(x)\n",
    "\n",
    "    # patches\n",
    "    x = PatchEmbedding(patch_size=4, embed_dim=64)(x)\n",
    "\n",
    "    # positional embeddings\n",
    "    x = PositionalEmbedding(max_len=1024, embed_dim=64)(x)\n",
    "\n",
    "    # transformer blocks\n",
    "    for _ in range(3):\n",
    "        x = TransformerEncoder(\n",
    "            embed_dim=64,\n",
    "            num_heads=2,\n",
    "            mlp_dim=128,\n",
    "            dropout=0.1\n",
    "        )(x)\n",
    "\n",
    "    # classifier head\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation=\"gelu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9b5a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_small_vit()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(3e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "00a63f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e14d13a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m174s\u001b[0m 132ms/step - accuracy: 0.2967 - loss: 1.5777 - val_accuracy: 0.3157 - val_loss: 1.5497\n",
      "Epoch 2/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 224ms/step - accuracy: 0.3097 - loss: 1.5539 - val_accuracy: 0.3188 - val_loss: 1.5384\n",
      "Epoch 3/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 297ms/step - accuracy: 0.3129 - loss: 1.5467 - val_accuracy: 0.3166 - val_loss: 1.5597\n",
      "Epoch 4/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 335ms/step - accuracy: 0.3180 - loss: 1.5433 - val_accuracy: 0.3207 - val_loss: 1.5471\n",
      "Epoch 5/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 223ms/step - accuracy: 0.3141 - loss: 1.5473 - val_accuracy: 0.3218 - val_loss: 1.5402\n",
      "Epoch 6/20\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m240s\u001b[0m 190ms/step - accuracy: 0.3164 - loss: 1.5440 - val_accuracy: 0.3217 - val_loss: 1.5385\n"
     ]
    }
   ],
   "source": [
    "history_1 = model.fit(\n",
    "    train_ds.batch(32),\n",
    "    validation_data=val_ds.batch(32),\n",
    "    epochs=20,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0450121",
   "metadata": {},
   "source": [
    "### EfficientNetB0 - lightweight transformer-like CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c55ef3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (96, 96)\n",
    "\n",
    "def preprocess_ef(image, label):\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "train_ef = train_ds.map(preprocess_ef).shuffle(10000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "val_ef   = val_ds.map(preprocess_ef).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_ef  = test_ds.map(preprocess_ef).batch(32).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "20052261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_12\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_12\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resizing_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Resizing</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ resizing_9 (\u001b[38;5;33mResizing\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │     \u001b[38;5;34m4,049,571\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_20 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "base = tf.keras.applications.EfficientNetB0( # pretrained feature extractor\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(96, 96, 3),\n",
    "    pooling=\"avg\"\n",
    ")\n",
    "\n",
    "base.trainable = False  # freeze backbone\n",
    "\n",
    "model_ef = tf.keras.Sequential([\n",
    "    tf.keras.layers.Resizing(96, 96),\n",
    "    tf.keras.layers.Conv2D(3, (1,1)),  # grayscale -> RGB\n",
    "\n",
    "    base,\n",
    "\n",
    "    tf.keras.layers.Dense(256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model_ef.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "433bdf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ef.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8487c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=4,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f303b914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m334s\u001b[0m 258ms/step - accuracy: 0.3037 - loss: 1.5790 - val_accuracy: 0.3113 - val_loss: 1.5687\n",
      "Epoch 2/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 375ms/step - accuracy: 0.3088 - loss: 1.5718 - val_accuracy: 0.3113 - val_loss: 1.5675\n",
      "Epoch 3/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 526ms/step - accuracy: 0.3133 - loss: 1.5689 - val_accuracy: 0.3113 - val_loss: 1.5676\n",
      "Epoch 4/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10710s\u001b[0m 8s/step - accuracy: 0.3079 - loss: 1.5716 - val_accuracy: 0.3113 - val_loss: 1.5695\n",
      "Epoch 5/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m720s\u001b[0m 570ms/step - accuracy: 0.3113 - loss: 1.5704 - val_accuracy: 0.3113 - val_loss: 1.5688\n",
      "Epoch 6/30\n",
      "\u001b[1m1264/1264\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4621s\u001b[0m 4s/step - accuracy: 0.3072 - loss: 1.5719 - val_accuracy: 0.3113 - val_loss: 1.5688\n"
     ]
    }
   ],
   "source": [
    "history_ef = model_ef.fit(\n",
    "    train_ef,\n",
    "    validation_data=val_ef,\n",
    "    epochs=30,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69107a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d34a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvesta see ja siis võrdle ka versiooniga, kus unfreezid top efficientNET layerid\n",
    "# või pigem tee see võrdlus oma tese transfer mudeliga sest seal tuli peale osalist unfreezimist nii hea tulemus\n",
    "\n",
    "base.trainable = True\n",
    "\n",
    "# Unfreeze only the top half to avoid overfitting\n",
    "for layer in base.layers[:150]:  \n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),  # lower LR for fine-tuning\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "history_ft = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=[callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fac99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
